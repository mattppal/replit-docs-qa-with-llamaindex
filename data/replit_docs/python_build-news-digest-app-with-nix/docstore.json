{"docstore/data": {"eeb5cf49-b966-40e1-a4c1-33ad00811ba4": {"__data__": {"id_": "eeb5cf49-b966-40e1-a4c1-33ad00811ba4", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06b7d3e6-1ef5-438c-946b-fdb41130ed5a", "node_type": "1", "metadata": {}, "hash": "9aa2a1bcc9bb1ff3e43a03c2f3922ec9108d0282a7298f09d4e36eb9ef98a624", "class_name": "RelatedNodeInfo"}}, "hash": "37f9262582540f2be1967204ed24cea37af6872a3327b354c20e967d21e68087", "text": "The second section of the script starts MongoDB with the following configuration options:\n\n--fork: This runs MongoDB in a background process, allowing the script to continue executing without shutting it down.\n\n--bind_ip=\"127.0.0.1\": Listen on the local loopback address only, preventing external access to our database.\n\n--dbpath=./data and --logpath=./log/mongod.log: Use local directories for storage. This is important for getting programs to work in Nix repls, as we discussed in our previous tutorial on building with Nix.\n\nThe third section starts Redis. We use the --bind flag to listen on the local loopback address only, similar to how we used it for MongoDB, and --daemonize yes runs it as a background process (similar to MongoDB's --fork).\n\nBefore we run our repl, we'll need to create our MongoDB data and logging directories, data and log. Create these directories now in your repl's filepane.\n\nOnce that's done, you can run your repl, and it will start MongoDB and Redis. You can interact with MongoDB by running mongo in your repl's shell, and with Redis by running redis-cli. If you're interested, you can find an introduction to these clients at the links below:\n\nWorking with the mongo Shell\n\nredis-cli, the Redis command line interface\n\nThese datastores will be empty for now.\n\nImportant note: Sometimes, when stopping and starting your repl, you may see the following error message:\n\nERROR: child process failed, exited with error number\n\n100\n\nThis means that MongoDB has failed to start. If you see this, restart your repl, and MongoDB should start up successfully.\n\nScraping RSS and Atom feeds\u200b\n\nWe're going to build the feed scraper first. If you've completed any of our previous web-scraping tutorials, you might expect to do this by parsing raw XML with Beautiful Soup. While this would be possible, we would need to account for a large number of differences in feed formats and other gotchas specific to parsing RSS and Atom feeds. Instead, we'll use the feedparser library, which has already solved most of these problems.\n\nCreate a directory named lib, and inside that directory, a Python file named scraper.py. Add the following code to it:\n\nimport\n\nfeedparser\n\npytz\n\ntime\n\nfrom\n\ndatetime\n\nimport\n\ndatetime\n\ntimedelta\n\nfrom\n\ndateutil\n\nimport\n\nparser\n\ndef\n\nget_title\n\nfeed_url\n\npass\n\ndef\n\nget_items\n\nfeed_url\n\nsince\n\ntimedelta\n\ndays\n\npass\n\nHere we import the libraries we'll need for web scraping, XML parsing, and time handling. We also define two functions:\n\nget_title: This will return the name of the website, for a given feed track (e.g. \"Hacker News\" for https://news.ycombinator.com/rss).\n\nget_items: This will return the feed's items \u2013 depending on the feed, these can be articles, videos, podcast episodes, or other content. The since parameter will allow us to only fetch recent content, and we'll use one day as the default cutoff.\n\nEdit the get_title function with the following:\n\ndef\n\nget_title\n\nfeed_url\n\nfeed\n\nfeedparser\n\nparse\n\nfeed_url\n\nreturn\n\nfeed\n\n\"feed\"\n\n\"title\"\n\nAdd the following line to the bottom of scraper.py to test it out:\n\nprint\n\nget_title\n\n\"https://news.ycombinator.com/rss\"\n\nInstead of rewriting our start.sh script to run this Python file, we can just run python lib/scraper.py in our repl's shell tab, as shown below. If it's working correctly, we should see \"Hacker News\" as the script's output.\n\nNow we need to write the second function. Add the following code to the get_items function definition:\n\ndef\n\nget_items\n\nfeed_url\n\nsince\n\ntimedelta\n\ndays\n\nfeed\n\nfeedparser\n\nparse\n\nfeed_url\n\nitems\n\nfor\n\nentry\n\nin\n\nfeed\n\nentries\n\ntitle\n\nentry\n\ntitle\n\nlink\n\nentry\n\nlink\n\nif\n\n\"published\"\n\nin\n\nentry\n\npublished\n\nparser\n\nparse\n\nentry\n\npublished\n\nelif\n\n\"pubDate\"\n\nin\n\nentry\n\npublished\n\nparser\n\nparse\n\nentry\n\npubDate\n\nHere we extract each item's title, link, and publishing timestamp. Atom feeds use the published element and RSS feeds use the pubDate element, so we look for both. We use parser to convert the timestamp from a string to a datetime object. The parse function is able to convert a large number of different formats, which saves us from writing a lot of extra code.\n\nWe need to evaluate the age of the content and package it in a dictionary so we can return it from our function.", "start_char_idx": 0, "end_char_idx": 4247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06b7d3e6-1ef5-438c-946b-fdb41130ed5a": {"__data__": {"id_": "06b7d3e6-1ef5-438c-946b-fdb41130ed5a", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eeb5cf49-b966-40e1-a4c1-33ad00811ba4", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "37f9262582540f2be1967204ed24cea37af6872a3327b354c20e967d21e68087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f46533e4-fcd5-484e-8c70-11507ebe4b71", "node_type": "1", "metadata": {}, "hash": "470ee04a24cf7986d1f2c2168139c279391182234fd374022f39365022956188", "class_name": "RelatedNodeInfo"}}, "hash": "9aa2a1bcc9bb1ff3e43a03c2f3922ec9108d0282a7298f09d4e36eb9ef98a624", "text": "Atom feeds use the published element and RSS feeds use the pubDate element, so we look for both. We use parser to convert the timestamp from a string to a datetime object. The parse function is able to convert a large number of different formats, which saves us from writing a lot of extra code.\n\nWe need to evaluate the age of the content and package it in a dictionary so we can return it from our function. Add the following code to the bottom of the get_items function:\n\n# evaluating content age\n\nif\n\nsince\n\nand\n\npublished\n\npytz\n\nutc\n\nlocalize\n\ndatetime\n\ntoday\n\nsince\n\nor\n\nnot\n\nsince\n\nitem\n\n\"title\"\n\ntitle\n\n\"link\"\n\nlink\n\n\"published\"\n\npublished\n\nitems\n\nappend\n\nitem\n\nreturn\n\nitems\n\nFinally, we construct a dictionary of our item's data and add it to the items list, which we return at the bottom of the function, outside the for loop.\n\nAdd the following lines to the bottom of scraper.py and run the script in your repl's shell again. We use time.sleep to avoid being rate-limited for fetching the same file twice in quick succession.\n\ntime\n\nsleep\n\nprint\n\nget_items\n\n\"https://news.ycombinator.com/rss\"\n\nYou should see a large number of results in your terminal. Play around with values of since and see what difference it makes.\n\nOnce you're done, remove the print statements from the bottom of the file. We've now built our feed scraper, which we'll use as a library in our main application.\n\nSetting up Mailgun\u200b\n\nNow that we can retrieve content for our email digests, we need a way of sending emails. To avoid having to set up our own email server, we'll use the Mailgun API to actually send emails. Sign up for a free account now, and verify your email and phone number.\n\nOnce your account is created and verified, you'll need an API key and domain from Mailgun.\n\nTo find your domain, navigate to Sending \u2192 Domains. You should see a single domain name, starting with \"sandbox\". Click on that and copy the full domain name (it looks like: sandboxlongstringoflettersandnumbers.mailgun.org).\n\nTo find your API key, navigate to Settings \u2192 API Keys. Click on the view icon next to Private API key and copy the revealed string somewhere safe.\n\nBack in your repl, create two environment variables, MAILGUN_DOMAIN and MAILGUN_APIKEY, and provide the strings you copied from Mailgun as values for each.\n\nRun your repl now to set these environment variables. Then create a file named lib/tasks.py, and populate it with the code below.\n\nimport\n\nrequests\n\nos\n\n# Mailgun config\n\nMAILGUN_APIKEY\n\nos\n\nenviron\n\n\"MAILGUN_APIKEY\"\n\nMAILGUN_DOMAIN\n\nos\n\nenviron\n\n\"MAILGUN_DOMAIN\"\n\ndef\n\nsend_test_email\n\nto_address\n\nres\n\nrequests\n\npost\n\nf\"https://api.mailgun.net/v3/\n\nMAILGUN_DOMAIN\n\n/messages\"\n\nauth\n\n\"api\"\n\nMAILGUN_APIKEY\n\ndata\n\n\"from\"\n\nf\"News Digest <digest@\n\nMAILGUN_DOMAIN\n\n>\"\n\n\"to\"\n\nto_address\n\n\"subject\"\n\n\"Testing Mailgun\"\n\n\"text\"\n\n\"Hello world!\"\n\nprint\n\nres\n\nsend_test_email\n\n\"YOUR-EMAIL-ADDRESS-HERE\"\n\nHere we use Python Requests to interact with the Mailgun API. Note the inclusion of our domain and API key.\n\nTo test that Mailgun is working, replace YOUR-EMAIL-ADDRESS-HERE with your email address, and then run python lib/tasks.py in your repl's shell. You should receive a test mail within a few minutes, but as we're using a free sandbox domain, it may end up in your spam folder.\n\nWithout further verification on Mailgun, we can only send up to 100 emails per hour, and a free account limits us to 5,000 emails per month. Additionally, Mailgun's sandbox domains can only be used to send emails to specific, whitelisted addresses. The address you created your account with will work, but if you want to send emails to other addresses, you'll have to add them to the domain's authorized recipients, which can be done from the page you got the full domain name from. Keep these limitations in mind as you build and test this application.\n\nAfter you've received your test email, you can delete or comment out the function call in the final line of lib/tasks.py.", "start_char_idx": 3838, "end_char_idx": 7795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f46533e4-fcd5-484e-8c70-11507ebe4b71": {"__data__": {"id_": "f46533e4-fcd5-484e-8c70-11507ebe4b71", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06b7d3e6-1ef5-438c-946b-fdb41130ed5a", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "9aa2a1bcc9bb1ff3e43a03c2f3922ec9108d0282a7298f09d4e36eb9ef98a624", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51fb1699-db70-45c6-8668-d969e1b9c811", "node_type": "1", "metadata": {}, "hash": "08c9b54172019f402fc3e6078ea162c754c377eebebba9586bbde19b1620d389", "class_name": "RelatedNodeInfo"}}, "hash": "470ee04a24cf7986d1f2c2168139c279391182234fd374022f39365022956188", "text": "You should receive a test mail within a few minutes, but as we're using a free sandbox domain, it may end up in your spam folder.\n\nWithout further verification on Mailgun, we can only send up to 100 emails per hour, and a free account limits us to 5,000 emails per month. Additionally, Mailgun's sandbox domains can only be used to send emails to specific, whitelisted addresses. The address you created your account with will work, but if you want to send emails to other addresses, you'll have to add them to the domain's authorized recipients, which can be done from the page you got the full domain name from. Keep these limitations in mind as you build and test this application.\n\nAfter you've received your test email, you can delete or comment out the function call in the final line of lib/tasks.py.\n\nInteracting with MongoDB\u200b\n\nAs we will have two different components of our application interacting with our Mongo database \u2013 our email-sending code in lib/tasks.py and the web application code we will put in main.py \u2013 we're going to put our database connection code in another file, which can be imported by both. Create lib/db.py now and add the following code to it:\n\nimport\n\npymongo\n\ndef\n\nconnect_to_db\n\nclient\n\npymongo\n\nMongoClient\n\nreturn\n\nclient\n\ndigest\n\nWe will call connect_to_db() whenever we need to interact with the database. Because of how MongoDB works, a new database called \"digest\" will be created the first time we connect. Much of the benefit MongoDB provides over traditional SQL databases is that you don't have to define schemas before storing data.\n\nMongo databases are made up of collections, which contain documents. You can think of the collections as lists and the documents as dictionaries. When we read and write data to and from MongoDB, we will be working with lists of dictionaries.\n\nCreating the web application\u200b\n\nNow that we've got a working webscraper, email sender and database interface, it's time to start building our web application.\n\nCreate a file named main.py in your repl's filepane and add the following import code to it:\n\nfrom\n\nflask\n\nimport\n\nFlask\n\nrequest\n\nrender_template\n\nsession\n\nflash\n\nredirect\n\nurl_for\n\nfrom\n\nfunctools\n\nimport\n\nwraps\n\nimport\n\nos\n\npymongo\n\ntime\n\nimport\n\nlib\n\nscraper\n\nas\n\nscraper\n\nimport\n\nlib\n\ntasks\n\nas\n\ntasks\n\nfrom\n\nlib\n\ndb\n\nimport\n\nconnect_to_db\n\nWe've imported everything we'll need from Flask and other Python modules, as well as our three local files from lib: scraper.py, tasks.py and db.py. Next, add the following code to initialize the application and connect to the database:\n\napp\n\nFlask\n\n__name__\n\napp\n\nconfig\n\n'SECRET_KEY'\n\nos\n\nenviron\n\n'SECRET_KEY'\n\ndb\n\nconnect_to_db\n\nOur secret key will be a long, random string, stored in an environment variable. You can generate one in your repl's Python console with the following two lines of code:\n\nimport\n\nrandom\n\nstring\n\n''\n\njoin\n\nrandom\n\nSystemRandom\n\nchoice\n\nstring\n\nascii_uppercase\n\nstring\n\ndigits\n\nfor\n\nin\n\nrange\n\n20\n\nIn your repl's \"Secrets\" tab, add a new key named SECRET_KEY and enter the random string you just generated as its value.\n\nNext, we will create the context helper function. This function will provide the current user's data from our database to our application frontend. Add the following code to the bottom of main.py:\n\ndef\n\ncontext\n\nemail\n\nsession\n\n\"email\"\n\nif\n\n\"email\"\n\nin\n\nsession\n\nelse\n\nNone\n\ncursor\n\ndb\n\nsubscriptions\n\nfind\n\n\"email\"\n\nemail\n\nsubscriptions\n\nsubscription\n\nfor\n\nsubscription\n\nin\n\ncursor\n\nreturn\n\n\"user_email\"\n\nemail\n\n\"user_subscriptions\"\n\nsubscriptions\n\nWhen we build our user login, we will store the current user's email address in Flask's session object, which corresponds to a cookie that will be cryptographically signed with the secret key we defined above. Without this, users would be able to impersonate each other by changing their cookie data.\n\nWe query our MongoDB database by calling db.<name of collection>.find(). If we call find() without any arguments, all items in our collection will be returned. If we call find() with an argument, as we've done above, it will return results with keys and values that match our argument. The find() method returns a Cursor object, which we can extract the results of our query from.\n\nNext, we need to create an authentication function decorator, which will restrict parts of our application to logged-in users.", "start_char_idx": 6988, "end_char_idx": 11330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51fb1699-db70-45c6-8668-d969e1b9c811": {"__data__": {"id_": "51fb1699-db70-45c6-8668-d969e1b9c811", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f46533e4-fcd5-484e-8c70-11507ebe4b71", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "470ee04a24cf7986d1f2c2168139c279391182234fd374022f39365022956188", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "073b4e68-46fc-42dc-8cc2-8bce80bf6c96", "node_type": "1", "metadata": {}, "hash": "b9bb18b825d340221e124df23be80c3a84c608495d81db53ae3452b6da9b9494", "class_name": "RelatedNodeInfo"}}, "hash": "08c9b54172019f402fc3e6078ea162c754c377eebebba9586bbde19b1620d389", "text": "Without this, users would be able to impersonate each other by changing their cookie data.\n\nWe query our MongoDB database by calling db.<name of collection>.find(). If we call find() without any arguments, all items in our collection will be returned. If we call find() with an argument, as we've done above, it will return results with keys and values that match our argument. The find() method returns a Cursor object, which we can extract the results of our query from.\n\nNext, we need to create an authentication function decorator, which will restrict parts of our application to logged-in users. Add the following code below the definition of the context function:\n\n# Authentication decorator\n\ndef\n\nauthenticated\n\n@wraps\n\ndef\n\ndecorated_function\n\nargs\n\n**\n\nkwargs\n\nif\n\n\"email\"\n\nnot\n\nin\n\nsession\n\nflash\n\n\"Permission denied.\"\n\n\"warning\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nreturn\n\nargs\n\n**\n\nkwargs\n\nreturn\n\ndecorated_function\n\nThe code in the second function may look a bit strange if you haven't written your own decorators before. Here's how it works: authenticated is the name of our decorator. You can think of decorators as functions that take other functions as arguments. (The two code snippets below are for illustration and not part of our program.) Therefore, if we write the following:\n\n@authenticated\n\ndef\n\nauthenticated_function\n\nreturn\n\nf\"Hello logged-in user!\"\n\nauthenticated_function\n\nIt will be roughly equivalent to:\n\ndef\n\nauthenticated_function\n\nreturn\n\nf\"Hello logged-in user!\"\n\nauthenticated\n\nauthenticated_function\n\nSo whenever authenticated_function gets called, the code we've defined in decorated_function will execute before anything we define in authenticated_function. This means we don't have to include the same authentication checking code in every piece of authenticated functionality. As per the code, if a non-logged-in user attempts to access restricted functionality, our app will flash a warning message and redirect them to the home page.\n\nNext, we'll add code to serve our home page and start our application:\n\n# Routes\n\n@app\n\nroute\n\n\"/\"\n\ndef\n\nindex\n\nreturn\n\nrender_template\n\n\"index.html\"\n\n**\n\ncontext\n\napp\n\nrun\n\nhost\n\n'0.0.0.0'\n\nport\n\n8080\n\nThis code will serve a Jinja template, which we will create now in a separate file. In your repl's filepane, create a directory named templates, and inside that directory, a file named index.html. Add the following code to index.html:\n\n<!\n\nDOCTYPE\n\nhtml\n\nhtml\n\nhead\n\ntitle\n\nNews Digest\n\n</\n\ntitle\n\n</\n\nhead\n\nbody\n\n{% with messages = get_flashed_messages() %} {% if messages %}\n\nul\n\nclass\n\nflashes\n\n{% for message in messages %}\n\nli\n\n{{ message }}\n\n</\n\nli\n\n{% endfor %}\n\n</\n\nul\n\n{% endif %} {% endwith %} {% if user_email == None %}\n\nPlease enter your email to sign up/log in:\n\n</\n\nform\n\naction\n\n/login\n\nmethod\n\npost\n\ninput\n\ntype\n\ntext\n\nname\n\nemail\n\n/>\n\ninput\n\ntype\n\nsubmit\n\nvalue\n\nLogin\n\n/>\n\n</\n\nform\n\n{% else %}\n\nLogged in as {{ user_email }}.\n\n</\n\nh1\n\nSubscriptions\n\n</\n\nh1\n\nul\n\n{% for subscription in user_subscriptions %}\n\nli\n\nhref\n\n{{ subscription.url }}\n\n{{ subscription.title }}\n\n</\n\nform\n\naction\n\n/unsubscribe\n\nmethod\n\npost\n\nstyle\n\ndisplay\n\ninline\n\ninput\n\ntype\n\nhidden\n\nname\n\nfeed_url\n\nvalue\n\n{{subscription.url}}\n\n/>\n\ninput\n\ntype\n\nsubmit\n\nvalue\n\nUnsubscribe\n\n/>\n\n</\n\nform\n\n</\n\nli\n\n{% endfor %}\n\n</\n\nul\n\nAdd a new subscription:\n\n</\n\nform\n\naction\n\n/subscribe\n\nmethod\n\npost\n\ninput\n\ntype\n\ntext\n\nname\n\nfeed_url\n\n/>\n\ninput\n\ntype\n\nsubmit\n\nvalue\n\nSubscribe\n\n/>\n\n</\n\nform\n\nSend digest to your email now:\n\n</\n\nform\n\naction\n\n/send-digest\n\nmethod\n\npost\n\ninput\n\ntype\n\nsubmit\n\nvalue\n\nSend digest\n\n/>\n\n</\n\nform\n\nChoose a time to send your daily digest (must be UTC):\n\n</\n\nform\n\naction\n\n/schedule-digest\n\nmethod\n\npost\n\ninput\n\ntype\n\ntime\n\nname\n\ndigest_time\n\n/>\n\ninput\n\ntype\n\nsubmit\n\nvalue\n\nSchedule digest\n\n/>\n\n</\n\nform\n\n{% endif %}\n\n</\n\nbody\n\n</\n\nhtml\n\nAs this will be our application's only page, it contains a lot of functionality. From top to bottom:\n\nWe've included code to display flashed messages at the top of the page. This allows us to show users the results of their actions without creating additional pages.", "start_char_idx": 10730, "end_char_idx": 14827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "073b4e68-46fc-42dc-8cc2-8bce80bf6c96": {"__data__": {"id_": "073b4e68-46fc-42dc-8cc2-8bce80bf6c96", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51fb1699-db70-45c6-8668-d969e1b9c811", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "08c9b54172019f402fc3e6078ea162c754c377eebebba9586bbde19b1620d389", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fce2976-be8e-4532-b31b-d8ac4a40026b", "node_type": "1", "metadata": {}, "hash": "251b7c7ce44e7ba34dbdd1e4c96be8259fae90be4ae2816a403c219977a50d46", "class_name": "RelatedNodeInfo"}}, "hash": "b9bb18b825d340221e124df23be80c3a84c608495d81db53ae3452b6da9b9494", "text": "From top to bottom:\n\nWe've included code to display flashed messages at the top of the page. This allows us to show users the results of their actions without creating additional pages.\n\nIf the current user is not logged in, we display a login form.\n\nIf the current user is logged in, we display:A list of their current subscriptions, with an unsubscribe button next to each one.A form for adding new subscriptions.A button to send an email digest immediately.A form for sending email digests at a specific time each day.\n\nTo start our application when our repl runs, we must add an additional line to the bottom of start.sh:\n\n# Run Flask app\n\npython main.py\n\nOnce that's done, run your repl. You should see a login form.\n\nAdding user login\u200b\n\nWe will implement user login by sending a single-use login link to the email address provided in the login form. This provides a number of benefits:\n\nWe can use the code we've already written for sending emails.\n\nWe don't need to implement user registration separately.\n\nWe can avoid worrying about user passwords.\n\nTo send login emails asynchronously, we'll set up a Celery task.\n\nIn main.py, add the following code for the /login route below the definition of index:\n\n# Login\n\n@app\n\nroute\n\n\"/login\"\n\nmethods\n\n'POST'\n\ndef\n\nlogin\n\nemail\n\nrequest\n\nform\n\n'email'\n\ntasks\n\nsend_login_email\n\ndelay\n\nemail\n\nflash\n\n\"Check your email for a magic login link!\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nIn this function, we get the user's email, and pass it to a function we will define in lib/tasks.py. As this function will be a Celery task rather than a conventional function, we must call it with .delay(), a function in Celery's task-calling API.\n\nLet's implement this task now. Open lib/tasks.py and modify it as follows:\n\nimport\n\nrequests\n\nos\n\nimport\n\nrandom\n\nstring\n\n# NEW IMPORTS\n\nfrom\n\ncelery\n\nimport\n\nCelery\n\n# NEW IMPORT\n\nfrom\n\ncelery\n\nschedules\n\nimport\n\ncrontab\n\n# NEW IMPORT\n\nfrom\n\ndatetime\n\nimport\n\ndatetime\n\n# NEW IMPORT\n\nimport\n\nlib\n\nscraper\n\nas\n\nscraper\n\n# NEW IMPORT\n\nfrom\n\nlib\n\ndb\n\nimport\n\nconnect_to_db\n\n# NEW IMPORT\n\n# NEW LINE BELOW\n\nREPL_URL\n\nf\"https://\n\nos\n\nenviron\n\n'REPL_SLUG'\n\n--\n\nos\n\nenviron\n\n'REPL_OWNER'\n\n.repl.co\"\n\n# NEW LINES BELOW\n\n# Celery configuration\n\nCELERY_BROKER_URL\n\n\"redis://127.0.0.1:6379/0\"\n\nCELERY_BACKEND_URL\n\n\"redis://127.0.0.1:6379/0\"\n\ncelery\n\nCelery\n\n\"tasks\"\n\nbroker\n\nCELERY_BROKER_URL\n\nbacked\n\nCELERY_BACKEND_URL\n\ncelery\n\nconf\n\nenable_utc\n\nTrue\n\n# Mailgun config\n\nMAILGUN_APIKEY\n\nos\n\nenviron\n\n\"MAILGUN_APIKEY\"\n\nMAILGUN_DOMAIN\n\nos\n\nenviron\n\n\"MAILGUN_DOMAIN\"\n\n# NEW FUNCTION DECORATOR\n\n@celery\n\ntask\n\ndef\n\nsend_test_email\n\nto_address\n\nres\n\nrequests\n\npost\n\nf\"https://api.mailgun.net/v3/\n\nMAILGUN_DOMAIN\n\n/messages\"\n\nauth\n\n\"api\"\n\nMAILGUN_APIKEY\n\ndata\n\n\"from\"\n\nf\"News Digest <digest@\n\nMAILGUN_DOMAIN\n\n>\"\n\n\"to\"\n\nto_address\n\n\"subject\"\n\n\"Testing Mailgun\"\n\n\"text\"\n\n\"Hello world!\"\n\nprint\n\nres\n\n# COMMENT OUT THE TESTING LINE\n\n# send_test_email(\"YOUR-EMAIL-ADDRESS-HERE\")\n\nNote: Please refer to these docs to ensure that you are using the correct repl.co domain format.\n\nWe've added the following:\n\nAdditional imports for Celery and our other local files.\n\nA REPL_URL variable containing our repl's URL, which we construct using environment variables defined in every repl.\n\nInstantiation of a Celery object, configured to use Redis as a message broker and data backend, and the UTC timezone.\n\nA function decorator which converts our send_test_email function into a Celery task.\n\nNext, we'll define a function to generate unique IDs for our login links. Add the following code below the send_test_email function definition:\n\ndef\n\ngenerate_login_id\n\nreturn\n\n''\n\njoin\n\nrandom\n\nSystemRandom\n\nchoice\n\nstring\n\nascii_uppercase\n\nstring\n\ndigits\n\nfor\n\nin\n\nrange\n\n30\n\nThis code is largely similar to the code we used to generate our secret key.\n\nNext, we'll create the task we called in main.py: send_login_email.", "start_char_idx": 14642, "end_char_idx": 18513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fce2976-be8e-4532-b31b-d8ac4a40026b": {"__data__": {"id_": "3fce2976-be8e-4532-b31b-d8ac4a40026b", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "073b4e68-46fc-42dc-8cc2-8bce80bf6c96", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "b9bb18b825d340221e124df23be80c3a84c608495d81db53ae3452b6da9b9494", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b990c5b4-5aa9-4b46-a70e-a18509b74f64", "node_type": "1", "metadata": {}, "hash": "452ede2df3e41916af20bb9ab8c7062056bd294978555d2ce6f6ea8cf60d81be", "class_name": "RelatedNodeInfo"}}, "hash": "251b7c7ce44e7ba34dbdd1e4c96be8259fae90be4ae2816a403c219977a50d46", "text": "We've added the following:\n\nAdditional imports for Celery and our other local files.\n\nA REPL_URL variable containing our repl's URL, which we construct using environment variables defined in every repl.\n\nInstantiation of a Celery object, configured to use Redis as a message broker and data backend, and the UTC timezone.\n\nA function decorator which converts our send_test_email function into a Celery task.\n\nNext, we'll define a function to generate unique IDs for our login links. Add the following code below the send_test_email function definition:\n\ndef\n\ngenerate_login_id\n\nreturn\n\n''\n\njoin\n\nrandom\n\nSystemRandom\n\nchoice\n\nstring\n\nascii_uppercase\n\nstring\n\ndigits\n\nfor\n\nin\n\nrange\n\n30\n\nThis code is largely similar to the code we used to generate our secret key.\n\nNext, we'll create the task we called in main.py: send_login_email. Add the following code below the definition of generate_login_id:\n\n@celery\n\ntask\n\ndef\n\nsend_login_email\n\nto_address\n\n# Generate ID\n\nlogin_id\n\ngenerate_login_id\n\n# Set up email\n\nlogin_url\n\nf\"\n\nREPL_URL\n\n/confirm-login/\n\nlogin_id\n\ntext\n\nf\"\"\"\n\nClick this link to log in:\n\nlogin_url\n\n\"\"\"\n\nhtml\n\nf\"\"\"\n\n<p>Click this link to log in:</p>\n\n<p><a href=\n\nlogin_url\n\nlogin_url\n\n</a></p>\n\n\"\"\"\n\n# Send email\n\nres\n\nrequests\n\npost\n\nf\"https://api.mailgun.net/v3/\n\nMAILGUN_DOMAIN\n\n/messages\"\n\nauth\n\n\"api\"\n\nMAILGUN_APIKEY\n\ndata\n\n\"from\"\n\nf\"News Digest <digest@\n\nMAILGUN_DOMAIN\n\n>\"\n\n\"to\"\n\nto_address\n\n\"subject\"\n\n\"News Digest Login Link\"\n\n\"text\"\n\ntext\n\n\"html\"\n\nhtml\n\n# Add to user_sessions collection if email sent successfully\n\nif\n\nres\n\nok\n\ndb\n\nconnect_to_db\n\ndb\n\nuser_sessions\n\ninsert_one\n\n\"login_id\"\n\nlogin_id\n\n\"email\"\n\nto_address\n\nprint\n\nf\"Sent login email to\n\nto_address\n\nelse\n\nprint\n\n\"Failed to send login email.\"\n\nThis code will generate a login ID, construct an email containing a /confirm-login link containing that ID, and then send the email. If the email is sent successfully, it will add a document to our MongoDB containing the email address and login ID.\n\nNow we can return to main.py and create the /confirm-login route. Add the following code below the login function definition:\n\n@app\n\nroute\n\n\"/confirm-login/<login_id>\"\n\ndef\n\nconfirm_login\n\nlogin_id\n\nlogin\n\ndb\n\nuser_sessions\n\nfind_one\n\n\"login_id\"\n\nlogin_id\n\nif\n\nlogin\n\nsession\n\n\"email\"\n\nlogin\n\n\"email\"\n\ndb\n\nuser_sessions\n\ndelete_one\n\n\"login_id\"\n\nlogin_id\n\n# prevent reuse\n\nelse\n\nflash\n\n\"Invalid or expired login link.\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nWhen a user clicks the login link in their email, they will be directed to this route. If a matching login ID is found in the database, they will be logged in, and the login ID will be deleted so it can't be reused.\n\nWe've implemented all of the code we need for user login. The last thing we need to do to get it working is to configure our repl to start a Celery worker. When we invoke a task with .delay(), this worker will execute the task.\n\nIn start.sh, add the following between the line that starts Redis and the line that starts our web application:\n\n# Run Celery worker\n\ncelery\n\nA lib\n\ntasks\n\ncelery worker\n\nP processes\n\nloglevel\n\ninfo\n\nThis will start a Celery worker, configured with the following flags:\n\n-A lib.tasks.celery: This tells Celery to run tasks associated with the celery object in tasks.py.\n\n-P processes: This tells Celery to start new processes for individual tasks.\n\n--loglevel=info: This ensures we'll have detailed Celery logs to help us debug problems.\n\nWe use & to run the worker in the background \u2013 this is a part of Bash's syntax rather than a program-specific backgrounding flag like we used for MongoDB and Redis.\n\nRun your repl now, and you should see the worker start up with the rest of our application's components. Once the web application is started, open it in a new tab. Then try logging in with your email address \u2013 remember to check your spam box for your login email.\n\nIf everything's working correctly, you should see a page like this after clicking your login link:\n\nAdding and removing subscriptions\u200b\n\nNow that we can log in, let's add the routes that handle subscribing to and unsubscribing from news feeds.", "start_char_idx": 17681, "end_char_idx": 21777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b990c5b4-5aa9-4b46-a70e-a18509b74f64": {"__data__": {"id_": "b990c5b4-5aa9-4b46-a70e-a18509b74f64", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fce2976-be8e-4532-b31b-d8ac4a40026b", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "251b7c7ce44e7ba34dbdd1e4c96be8259fae90be4ae2816a403c219977a50d46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0c4093a-768c-4246-8c16-c0dc5be44105", "node_type": "1", "metadata": {}, "hash": "90ceb13a5e403b10c4bd36b55a57cb0b9f8d6de0f31cf5d430d97142c1329707", "class_name": "RelatedNodeInfo"}}, "hash": "452ede2df3e41916af20bb9ab8c7062056bd294978555d2ce6f6ea8cf60d81be", "text": "-P processes: This tells Celery to start new processes for individual tasks.\n\n--loglevel=info: This ensures we'll have detailed Celery logs to help us debug problems.\n\nWe use & to run the worker in the background \u2013 this is a part of Bash's syntax rather than a program-specific backgrounding flag like we used for MongoDB and Redis.\n\nRun your repl now, and you should see the worker start up with the rest of our application's components. Once the web application is started, open it in a new tab. Then try logging in with your email address \u2013 remember to check your spam box for your login email.\n\nIf everything's working correctly, you should see a page like this after clicking your login link:\n\nAdding and removing subscriptions\u200b\n\nNow that we can log in, let's add the routes that handle subscribing to and unsubscribing from news feeds. These routes will only be available to logged-in users, so we'll use our authenticated decorator on them. Add the following code below the confirm_login function definition in main.py:\n\n# Subscriptions\n\n@authenticated\n\n@app\n\nroute\n\n\"/subscribe\"\n\nmethods\n\n'POST'\n\ndef\n\nsubscribe\n\n# new feed\n\nfeed_url\n\nrequest\n\nform\n\n\"feed_url\"\n\n# Test feed\n\ntry\n\nitems\n\nscraper\n\nget_items\n\nfeed_url\n\nNone\n\nexcept\n\nException\n\nas\n\nprint\n\nflash\n\n\"Invalid feed URL.\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nif\n\nitems\n\n==\n\nflash\n\n\"Invalid feed URL\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\n# Get feed title\n\ntime\n\nsleep\n\nfeed_title\n\nscraper\n\nget_title\n\nfeed_url\n\nThis code will validate feed URLs by attempting to fetch their contents. Note that we are passing None as the argument for since in scraper.get_items \u2013 this will fetch the whole feed, not just the last day's content. If it fails for any reason, or returns an empty list, an error message will be shown to the user and the subscription will not be added.\n\nOnce we're sure that the feed is valid, we sleep for one second and then fetch the title. The sleep is necessary to prevent rate-limiting by some websites.\n\nNow that we've validated the feed and have its title, we can add it to our MongoDB. Add the following code to the bottom of the function:\n\n# Add subscription to Mongodb\n\ntry\n\ndb\n\nsubscriptions\n\ninsert_one\n\n\"email\"\n\nsession\n\n\"email\"\n\n\"url\"\n\nfeed_url\n\n\"title\"\n\nfeed_title\n\nexcept\n\npymongo\n\nerrors\n\nDuplicateKeyError\n\nflash\n\n\"You're already subscribed to that feed.\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nexcept\n\nException\n\nflash\n\n\"An unknown error occured.\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\n# Create unique index if it doesn't exist\n\ndb\n\nsubscriptions\n\ncreate_index\n\n\"email\"\n\n\"url\"\n\nunique\n\nTrue\n\nflash\n\n\"Feed added!\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nHere, we populate a new document with our subscription details and insert it into our \"subscriptions\" collection. To prevent duplicate subscriptions, we use create_index to create a unique compound index on the \"email\" and \"url\" fields. As create_index will only create an index that doesn't already exist, we can safely call it on every invocation of this function.\n\nNext, we'll create the code for unsubscribing from feeds. Add the following function definition below the one above:\n\n@authenticated\n\n@app\n\nroute\n\n\"/unsubscribe\"\n\nmethods\n\n'POST'\n\ndef\n\nunsubscribe\n\n# remove feed\n\nfeed_url\n\nrequest\n\nform\n\n\"feed_url\"\n\ndeleted\n\ndb\n\nsubscriptions\n\ndelete_one\n\n\"email\"\n\nsession\n\n\"email\"\n\n\"url\"\n\nfeed_url\n\nflash\n\n\"Unsubscribed!\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nRun your repl, and try subscribing and unsubscribing from some feeds. You can use the following URLs to test:\n\nHacker News feed: https://news.ycombinator.com/rss\n\n/r/replit on Reddit feed: https://www.reddit.com/r/replit.rss\n\nSending digests\u200b\n\nOnce you've added some subscriptions, we can implement the /send-digest route. Add the following code below the definition of unsubscribe in main.py:\n\n# Digest\n\n@authenticated\n\n@app\n\nroute\n\n\"/send-digest\"\n\nmethods\n\n'POST'\n\ndef\n\nsend_digest\n\ntasks\n\nsend_digest_email\n\ndelay\n\nsession\n\n\"email\"\n\nflash\n\n\"Digest email sent! Check your inbox.\"", "start_char_idx": 20936, "end_char_idx": 24923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0c4093a-768c-4246-8c16-c0dc5be44105": {"__data__": {"id_": "c0c4093a-768c-4246-8c16-c0dc5be44105", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b990c5b4-5aa9-4b46-a70e-a18509b74f64", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "452ede2df3e41916af20bb9ab8c7062056bd294978555d2ce6f6ea8cf60d81be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58b93ee6-37d1-4ce5-9dba-9a037220991a", "node_type": "1", "metadata": {}, "hash": "71508397df30290f0ce49e304fd8e9dbd07c8483deb9bba688586d7d6cf5e024", "class_name": "RelatedNodeInfo"}}, "hash": "90ceb13a5e403b10c4bd36b55a57cb0b9f8d6de0f31cf5d430d97142c1329707", "text": "return\n\nredirect\n\nurl_for\n\n\"index\"\n\nRun your repl, and try subscribing and unsubscribing from some feeds. You can use the following URLs to test:\n\nHacker News feed: https://news.ycombinator.com/rss\n\n/r/replit on Reddit feed: https://www.reddit.com/r/replit.rss\n\nSending digests\u200b\n\nOnce you've added some subscriptions, we can implement the /send-digest route. Add the following code below the definition of unsubscribe in main.py:\n\n# Digest\n\n@authenticated\n\n@app\n\nroute\n\n\"/send-digest\"\n\nmethods\n\n'POST'\n\ndef\n\nsend_digest\n\ntasks\n\nsend_digest_email\n\ndelay\n\nsession\n\n\"email\"\n\nflash\n\n\"Digest email sent! Check your inbox.\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nThen, in tasks.py, add the following new Celery task:\n\n@celery\n\ntask\n\ndef\n\nsend_digest_email\n\nto_address\n\n# Get subscriptions from Mongodb\n\ndb\n\nconnect_to_db\n\ncursor\n\ndb\n\nsubscriptions\n\nfind\n\n\"email\"\n\nto_address\n\nsubscriptions\n\nsubscription\n\nfor\n\nsubscription\n\nin\n\ncursor\n\n# Scrape RSS feeds\n\nitems\n\nfor\n\nsubscription\n\nin\n\nsubscriptions\n\nitems\n\nsubscription\n\n\"title\"\n\nscraper\n\nget_items\n\nsubscription\n\n\"url\"\n\nFirst, we connect to the MongoDB and find all subscriptions created by the user we're sending to. We then construct a dictionary of scraped items for each feed URL.\n\nOnce that's done, it's time to create the email content. Add the following code to the bottom of send_digest_email function:\n\n# Build email digest\n\ntoday_date\n\ndatetime\n\ntoday\n\nstrftime\n\n\"%d %B %Y\"\n\nhtml\n\nf\"<h1>Daily Digest for\n\ntoday_date\n\n</h1>\"\n\nfor\n\nsite_title\n\nfeed_items\n\nin\n\nitems\n\nitems\n\nif\n\nnot\n\nfeed_items\n\n# empty list\n\ncontinue\n\nsection\n\nf\"<h2>\n\nsite_title\n\n</h2>\"\n\nsection\n\n+=\n\n\"<ul>\"\n\nfor\n\nitem\n\nin\n\nfeed_items\n\nsection\n\n+=\n\nf\"<li><a href=\n\nitem\n\n'link'\n\nitem\n\n'title'\n\n</a></li>\"\n\nsection\n\n+=\n\n\"</ul>\"\n\nhtml\n\n+=\n\nsection\n\nIn this code, we construct an HTML email with a heading and bullet list of linked items for each feed. If any of our feeds have no items for the last day, we leave them out of the digest. We use strftime to format today's date in a human-readable manner.\n\nAfter that, we can send the email. Add the following code to the bottom of the function:\n\n# Send email\n\nres\n\nrequests\n\npost\n\nf\"https://api.mailgun.net/v3/\n\nMAILGUN_DOMAIN\n\n/messages\"\n\nauth\n\n\"api\"\n\nMAILGUN_APIKEY\n\ndata\n\n\"from\"\n\nf\"News Digest <digest@\n\nMAILGUN_DOMAIN\n\n>\"\n\n\"to\"\n\nto_address\n\n\"subject\"\n\nf\"News Digest for\n\ntoday_date\n\n\"text\"\n\nhtml\n\n\"html\"\n\nhtml\n\nif\n\nres\n\nok\n\nprint\n\nf\"Sent digest email to\n\nto_address\n\nelse\n\nprint\n\n\"Failed to send digest email.\"\n\nRun your repl, and click on the Send digest button. You should receive an email digest with today's items from each of your subscriptions within a few minutes. Remember to check your spam!\n\nScheduling digests\u200b\n\nThe last thing we need to implement is scheduled digests, to allow our application to send users a digest every day at a specified time.\n\nIn main.py, add the following code below the send_digest function definition:\n\n@authenticated\n\n@app\n\nroute\n\n\"/schedule-digest\"\n\nmethods\n\n'POST'\n\ndef\n\nschedule_digest\n\n# Get time from form\n\nhour\n\nminute\n\nrequest\n\nform\n\n\"digest_time\"\n\nsplit\n\n\":\"\n\ntasks\n\nschedule_digest\n\nsession\n\n\"email\"\n\nint\n\nhour\n\nint\n\nminute\n\nflash\n\nf\"Your digest will be sent daily at\n\nhour\n\nminute\n\nUTC\"\n\nreturn\n\nredirect\n\nurl_for\n\n\"index\"\n\nThis function retrieves the requested digest time from the user and calls tasks.schedule_digest. As schedule_digest will be a regular function that schedules a task rather than a task itself, we can call it directly.\n\nCelery supports scheduling tasks through its beat functionality. This will require us to run an additional Celery process, which will be a beat rather than a worker.\n\nBy default, Celery does not support dynamic addition and alteration of scheduled tasks, which we need in order to allow users to set and change their digest schedules arbitrarily. So we'll need a custom scheduler that supports this.\n\nMany custom Celery scheduler packages are available on PyPI, but as of October 2021, none of these packages have been added to Nixpkgs.", "start_char_idx": 24306, "end_char_idx": 28303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58b93ee6-37d1-4ce5-9dba-9a037220991a": {"__data__": {"id_": "58b93ee6-37d1-4ce5-9dba-9a037220991a", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0c4093a-768c-4246-8c16-c0dc5be44105", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "90ceb13a5e403b10c4bd36b55a57cb0b9f8d6de0f31cf5d430d97142c1329707", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b937040a-afb2-4470-a994-a2036da1572c", "node_type": "1", "metadata": {}, "hash": "0800a7b28180c22dd29f239eef7e5f29bbef326a2d9d84b908af8f51d35c08c0", "class_name": "RelatedNodeInfo"}}, "hash": "71508397df30290f0ce49e304fd8e9dbd07c8483deb9bba688586d7d6cf5e024", "text": "As schedule_digest will be a regular function that schedules a task rather than a task itself, we can call it directly.\n\nCelery supports scheduling tasks through its beat functionality. This will require us to run an additional Celery process, which will be a beat rather than a worker.\n\nBy default, Celery does not support dynamic addition and alteration of scheduled tasks, which we need in order to allow users to set and change their digest schedules arbitrarily. So we'll need a custom scheduler that supports this.\n\nMany custom Celery scheduler packages are available on PyPI, but as of October 2021, none of these packages have been added to Nixpkgs. Therefore, we'll need to create a custom derivation for the scheduler we choose. Let's do that in replit.nix now. Open the file, and add the let ... in block below:\n\n{ pkgs }:\n\nlet\n\nredisbeat = pkgs.python39Packages.buildPythonPackage rec {\n\npname = \"redisbeat\";\n\nversion = \"1.2.4\";\n\nsrc = pkgs.python39Packages.fetchPypi {\n\ninherit pname version;\n\nsha256 = \"0b800c6c20168780442b575d583d82d83d7e9326831ffe35f763289ebcd8b4f6\";\n\n};\n\npropagatedBuildInputs = with pkgs.python39Packages; [\n\njsonpickle\n\ncelery\n\nredis\n\n];\n\npostPatch = ''\n\nsed -i \"s/jsonpickle==1.2/jsonpickle/\" setup.py\n\n'';\n\n};\n\nin\n\ndeps = [\n\npkgs.python39\n\npkgs.python39Packages.flask\n\npkgs.python39Packages.celery\n\npkgs.python39Packages.pymongo\n\npkgs.python39Packages.requests\n\npkgs.python39Packages.redis\n\npkgs.python39Packages.feedparser\n\npkgs.python39Packages.dateutil\n\npkgs.mongodb\n\npkgs.redis\n\nredisbeat # <-- ALSO ADD THIS LINE\n\n];\n\nWe've chosen to use redisbeat, as it is small, simple and uses Redis as a backend. We construct a custom derivation for it using the buildPythonPackage function, to which we pass the following information:\n\nThe package's name and version.\n\nsrc: Where to find the package's source code (in this case, from PyPI, but we could also use GitHub, or a generic URL).\n\npropagatedBuildInputs: The package's dependencies (all of which are available from Nixpkgs).\n\npostPatch: Actions to take before installing the package. For this package, we remove the version specification for dependency jsonpickle in setup.py. This will force redisbeat to use the latest version of jsonpickle, which is available from Nixpkgs and, as a bonus, does not contain this critical vulnerability.\n\nYou can learn more about using Python with Nixpkgs in this section of the official documentation.\n\nTo actually install redisbeat, we must also add it to our deps list. Once you've done that, run your repl. Building custom Nix derivations like this one often takes some time, so you may have to wait a while before your repl finishes loading the Nix environment.\n\nWhile we wait, let's import redisbeat in lib/tasks.py and create our schedule_digest function. Add the following code to the bottom of lib/tasks.py:\n\nfrom\n\nredisbeat\n\nscheduler\n\nimport\n\nRedisScheduler\n\nscheduler\n\nRedisScheduler\n\napp\n\ncelery\n\ndef\n\nschedule_digest\n\nemail\n\nhour\n\nminute\n\nscheduler\n\nadd\n\n**\n\n\"name\"\n\n\"digest-\"\n\nemail\n\n\"task\"\n\n\"lib.tasks.send_digest_email\"\n\n\"kwargs\"\n\n\"to_address\"\n\nemail\n\n\"schedule\"\n\ncrontab\n\nminute\n\nminute\n\nhour\n\nhour\n\nThis code uses redisbeat's RedisScheduler to schedule the execution of our send_digest_email task. Note that we've used the task's full path, with lib included: this is necessary when scheduling.\n\nWe've used Celery's crontab schedule type, which is highly suited to managing tasks that run at a certain time each day.\n\nIf a task with the same name already exists in the schedule, scheduler.add will update it rather than adding a new task. This means our users can change their digest time at will.\n\nNow that our code is in place, we can add a new Celery beat process to start.sh. Add the following line just after the line that starts the Celery worker:\n\ncelery -A lib.tasks.celery beat -S redisbeat.RedisScheduler --loglevel\n\ndebug\n\nNow run your repl. You can test this functionality out now by scheduling your digest about ten minutes in the future. If you want to receive regular digests, you will need to enable Deployments in your repl. Also, remember that all times must be specified in the UTC timezone.", "start_char_idx": 27646, "end_char_idx": 31800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b937040a-afb2-4470-a994-a2036da1572c": {"__data__": {"id_": "b937040a-afb2-4470-a994-a2036da1572c", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "cf9abec2c4ade63d9b69bdb43074686a249cd1e6e5bd002c073099c2a87315f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58b93ee6-37d1-4ce5-9dba-9a037220991a", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}, "hash": "71508397df30290f0ce49e304fd8e9dbd07c8483deb9bba688586d7d6cf5e024", "class_name": "RelatedNodeInfo"}}, "hash": "0800a7b28180c22dd29f239eef7e5f29bbef326a2d9d84b908af8f51d35c08c0", "text": "We've used Celery's crontab schedule type, which is highly suited to managing tasks that run at a certain time each day.\n\nIf a task with the same name already exists in the schedule, scheduler.add will update it rather than adding a new task. This means our users can change their digest time at will.\n\nNow that our code is in place, we can add a new Celery beat process to start.sh. Add the following line just after the line that starts the Celery worker:\n\ncelery -A lib.tasks.celery beat -S redisbeat.RedisScheduler --loglevel\n\ndebug\n\nNow run your repl. You can test this functionality out now by scheduling your digest about ten minutes in the future. If you want to receive regular digests, you will need to enable Deployments in your repl. Also, remember that all times must be specified in the UTC timezone.\n\nWhere next?\u200b\n\nWe've built a useful multi-component application, but its functionality is fairly rudimentary. If you'd like to keep working on this project, here are some ideas for next steps:\n\nSet up a custom domain with Mailgun to help keep your digest emails out of spam.\n\nFeed scraper optimization. Currently, we fetch the whole feed twice when adding a new subscription and have to sleep to avoid rate-limiting. The scraper could be optimized to fetch feed contents only once.\n\nIntelligent digest caching. If multiple users subscribe to the same feed and schedule their digests for the same time, we will unnecessarily fetch the same content for each one.\n\nMultiple digests per user. Users could configure different digests with different contents at different times.\n\nAllow users to schedule digests in their local timezones.\n\nStyling of both website and email content with CSS.\n\nA production WSGI and web server to improve the web application's performance, like we used in our previous tutorial on building with Nix.\n\nYou can find our repl below:\n\n< Previous: Card game with pygame\n\nNext: Snake game with PyGame >\n\nWas this helpful?\n\nLast updated on Nov 2, 2023\n\nGetting started\n\nInstalling dependencies\n\nScraping RSS and Atom feeds\n\nSetting up Mailgun\n\nInteracting with MongoDB\n\nCreating the web application\n\nAdding user login\n\nAdding and removing subscriptions\n\nSending digests\n\nScheduling digests\n\nWhere next?", "start_char_idx": 30986, "end_char_idx": 33221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"43642c2c-a252-4bd6-8c7a-ca600e67ffed": {"node_ids": ["eeb5cf49-b966-40e1-a4c1-33ad00811ba4", "06b7d3e6-1ef5-438c-946b-fdb41130ed5a", "f46533e4-fcd5-484e-8c70-11507ebe4b71", "51fb1699-db70-45c6-8668-d969e1b9c811", "073b4e68-46fc-42dc-8cc2-8bce80bf6c96", "3fce2976-be8e-4532-b31b-d8ac4a40026b", "b990c5b4-5aa9-4b46-a70e-a18509b74f64", "c0c4093a-768c-4246-8c16-c0dc5be44105", "58b93ee6-37d1-4ce5-9dba-9a037220991a", "b937040a-afb2-4470-a994-a2036da1572c"], "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/python/build-news-digest-app-with-nix.html"}}}, "docstore/metadata": {"eeb5cf49-b966-40e1-a4c1-33ad00811ba4": {"doc_hash": "37f9262582540f2be1967204ed24cea37af6872a3327b354c20e967d21e68087", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "06b7d3e6-1ef5-438c-946b-fdb41130ed5a": {"doc_hash": "9aa2a1bcc9bb1ff3e43a03c2f3922ec9108d0282a7298f09d4e36eb9ef98a624", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "f46533e4-fcd5-484e-8c70-11507ebe4b71": {"doc_hash": "470ee04a24cf7986d1f2c2168139c279391182234fd374022f39365022956188", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "51fb1699-db70-45c6-8668-d969e1b9c811": {"doc_hash": "08c9b54172019f402fc3e6078ea162c754c377eebebba9586bbde19b1620d389", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "073b4e68-46fc-42dc-8cc2-8bce80bf6c96": {"doc_hash": "b9bb18b825d340221e124df23be80c3a84c608495d81db53ae3452b6da9b9494", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "3fce2976-be8e-4532-b31b-d8ac4a40026b": {"doc_hash": "251b7c7ce44e7ba34dbdd1e4c96be8259fae90be4ae2816a403c219977a50d46", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "b990c5b4-5aa9-4b46-a70e-a18509b74f64": {"doc_hash": "452ede2df3e41916af20bb9ab8c7062056bd294978555d2ce6f6ea8cf60d81be", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "c0c4093a-768c-4246-8c16-c0dc5be44105": {"doc_hash": "90ceb13a5e403b10c4bd36b55a57cb0b9f8d6de0f31cf5d430d97142c1329707", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "58b93ee6-37d1-4ce5-9dba-9a037220991a": {"doc_hash": "71508397df30290f0ce49e304fd8e9dbd07c8483deb9bba688586d7d6cf5e024", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}, "b937040a-afb2-4470-a994-a2036da1572c": {"doc_hash": "0800a7b28180c22dd29f239eef7e5f29bbef326a2d9d84b908af8f51d35c08c0", "ref_doc_id": "43642c2c-a252-4bd6-8c7a-ca600e67ffed"}}}