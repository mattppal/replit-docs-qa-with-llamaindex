{"docstore/data": {"f3052cdf-9dd3-46c6-bd7f-cf497590c62a": {"__data__": {"id_": "f3052cdf-9dd3-46c6-bd7f-cf497590c62a", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "8d715231bc49bfaea69af4b71785c513792c7b0713c7f450d47abb3318ef354f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43b9f65c-88ca-40f2-ad82-0be6f7bc61fe", "node_type": "1", "metadata": {}, "hash": "f9aad8c73bf7a2fcc12706652fdc4280f7b3e1b6cdb9249377afee71dd78d9eb", "class_name": "RelatedNodeInfo"}}, "hash": "6b1498aff70c18e85d5c75fa41b3c99f46ff183e375ac52aa1e253ce61ca89f9", "text": "// disconected from surrounding words\n\nconst\n\nreplacements\n\n\",\"\n\n\".\"\n\n\":\"\n\n\"!\"\n\n\"?\"\n\n'\"'\n\n\"\u201c\"\n\n\"\u201d\"\n\n\";\"\n\n\"(\"\n\n\")\"\n\n\"-\"\n\n\"_\"\n\nreplacements\n\nforEach\n\nvalue\n\n=>\n\ndata\n\ndata\n\nreplace\n\nRegExp\n\n\"\\\\\"\n\nvalue\n\n\"g\"\n\n\" \"\n\nvalue\n\n\" \"\n\nThe line data = data.replace(/\\r?\\n|\\r/g, \" \"); uses a regular expression to look for the newline markers \\r\\n (on windows) and \\n on Unix, Linux and macOS.\n\nThe next few lines define all the punctuation we expect in our sources. Then, it searches the books for each punctuation mark, and replaces it with a leading space. For example, a question mark at the end of a sentence \"Is this working?\" would be modified to \"Is this working ? \".\n\nWith our punctuation neatly separated from each word, we must now look for the spaces between things to split our text into tokens. Let's add that code to our function, with the following few lines:\n\n// Split on spaces to get each word by itself, indexed.\n\nvar\n\nword_array\n\ndata\n\nsplit\n\n\" \"\n\n// remove all pure whitespace entries\n\nword_array\n\nword_array\n\nfilter\n\nword\n\n=>\n\nword\n\ntrim\n\nlength\n\n!=\n\nreturn\n\nword_array\n\nThis uses the string split function to split all the sentences into individual words, by looking for the spaces ' ' between them. Then we do a little cleanup to remove any resulting entries that are just pure whitespace.\n\nGreat! Now our function will take in a list of books, and convert each word or punctuation mark into an element in an array, like this:\n\n\"No\"\n\n\"one\"\n\n\"would\"\n\n\"have\"\n\n\"believed\"\n\n\"in\"\n\n\"the\"\n\n\"last\"\n\n\"years\"\n\n\"of\"\n\n\"the\"\n\n\"nineteenth\"\n\n\"century\"\n\n\"that\"\n\n\"this\"\n\n\"world\"\n\n\"was\"\n\n\"being\"\n\n\"watched\"\n\n\"keenly\"\n\n\"and\"\n\n\"closely\"\n\n\"by\"\n\n\"intelligences\"\n\n\"greater\"\n\n\"than\"\n\n\"man\"\n\n\"\u2019\"\n\n\"s\"\n\n\"and\"\n\n\"yet\"\n\n\"as\"\n\n\"mortal\"\n\n\"as\"\n\n\"his\"\n\n\"own\"\n\n\";\"\n\nCreating the Data Structure\u200b\n\nNow that we have all the books tokenized in an array, let's see how we can populate our proposed data structure with them. We'll create another function to deal with this, called buildMap. Add this to the index.js file:\n\nfunction\n\nbuildMap\n\ntokens\n\ndepth\n\nThe parameter tokens accepts, as an argument, the output of the file parsing function we created above. depth refers to how many tokens long the initial phrases should be.\n\nNow, let's think a bit about the algorithm we'll need to devise to extract the initial phrases, and the words that are likely to follow from our tokens. We need to go through the tokens, at depth amount at a time, in a kind of sliding window fashion to extract the initial phrases. You could visualize it like this:\n\nWe'll look in our structure to see if that phrase is already there \u2013 if not, we'll add it. Next we'll look at the word immediately after the phrase, and check if it is in the list of likely words for that phrase. If it is already there, increment its weight. If it's not already there, add it and set its weight to 1.\n\nIn pseudo-code, this could be expressed as:\n\nfor each entry in the tokens\n\ncreate a phrase from the current token and the next depth-1 number of tokens\n\nif the phrase doesn't already exist in the map\n\nadd the phrase to the map\n\nget the next token after the phrase (current token + depth index) as likely word\n\nif the word does not exist in the phrase word list\n\nadd word\n\nincrement word weight\n\nCool, let's add this as code to the function buildMap.", "start_char_idx": 0, "end_char_idx": 3287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43b9f65c-88ca-40f2-ad82-0be6f7bc61fe": {"__data__": {"id_": "43b9f65c-88ca-40f2-ad82-0be6f7bc61fe", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "8d715231bc49bfaea69af4b71785c513792c7b0713c7f450d47abb3318ef354f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3052cdf-9dd3-46c6-bd7f-cf497590c62a", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "6b1498aff70c18e85d5c75fa41b3c99f46ff183e375ac52aa1e253ce61ca89f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e20735d-9bb9-4f32-a344-b573b12211f4", "node_type": "1", "metadata": {}, "hash": "66d45d962b4a0ed01d599135a1902e7275f448cdac702fd52452465b98bb52a5", "class_name": "RelatedNodeInfo"}}, "hash": "f9aad8c73bf7a2fcc12706652fdc4280f7b3e1b6cdb9249377afee71dd78d9eb", "text": "You could visualize it like this:\n\nWe'll look in our structure to see if that phrase is already there \u2013 if not, we'll add it. Next we'll look at the word immediately after the phrase, and check if it is in the list of likely words for that phrase. If it is already there, increment its weight. If it's not already there, add it and set its weight to 1.\n\nIn pseudo-code, this could be expressed as:\n\nfor each entry in the tokens\n\ncreate a phrase from the current token and the next depth-1 number of tokens\n\nif the phrase doesn't already exist in the map\n\nadd the phrase to the map\n\nget the next token after the phrase (current token + depth index) as likely word\n\nif the word does not exist in the phrase word list\n\nadd word\n\nincrement word weight\n\nCool, let's add this as code to the function buildMap. It should look like this:\n\nlet\n\nmap\n\n// for each entry in the tokens\n\nfor\n\nlet\n\nindex\n\nindex\n\ntokens\n\nlength\n\ndepth\n\nindex\n\n++\n\n//create a phrase from the current token and the next depth-1 number of tokens\n\nlet\n\nphrase\n\n\"\"\n\nfor\n\nlet\n\ndepthIndex\n\ndepthIndex\n\ndepth\n\ndepthIndex\n\n++\n\nconst\n\ncurr_word\n\ntokens\n\ndepthIndex\n\nindex\n\nphrase\n\nphrase\n\n\" \"\n\ncurr_word\n\n// Get rid of any extra space we added in when constructing the phrase from tokens\n\nphrase\n\nphrase\n\ntrimStart\n\n//if the phrase doesn't already exist in the map\n\n//    add the phrase to the map, and add a blank\n\nif\n\nmap\n\nphrase\n\nmap\n\nphrase\n\n// Gets the next word after the phrase\n\nlet\n\nnext_word\n\ntokens\n\nindex\n\ndepth\n\n// See if the next word exists in the phrase word list\n\n// If it doesn't already exist in the possible next word list, add it in, and set weight to 1\n\n// if it does exist, just increment the weight\n\nlet\n\nnext_word_list\n\nmap\n\nphrase\n\nif\n\nnext_word_list\n\nnext_word\n\nnext_word_list\n\nnext_word\n\nelse\n\nnext_word_list\n\nnext_word\n\n++\n\nreturn\n\nmap\n\nIn line 1, we create an empty object using the literal notation to hold our data structure, which is a map between phrases and words that follow.\n\nIn line 4, we setup a for loop to run through each token. Notice that we only run up to the token's array length less the phrase depth. This is because we need to take into account that we have to get a word after the last phrase, so we must stop getting phrases before the end of the token array.\n\nThe rest of the code implements our pseudo code. The comments match the place in the pseudo code that the real code implements.\n\nNow we have a way to build up our data structure using our training text.\n\nCompleting a Phrase\u200b\n\nLet's use our data structure, along with an initial phrase, to pick out a suggestion/completion. There are two parts to this task:\n\nFind the matching phrase, and likely next words in our map.\n\nPick one of the likely words to follow the phrase.\n\nTo find the matching phrase and retrieve the likely word list, we can use the indexer functionality of JavaScript. Let's create a method to hold this logic. As inputs, we'll need the phrase to autocomplete, along with a populated map.\n\nfunction\n\nsuggest_word\n\nstart_phrase\n\nword_map\n\nlet\n\nword_list\n\nword_map\n\nstart_phrase\n\nWe now have the function definition, and we have retrieved the word list for the given phrase. We need to pick one of words from the list to return. Remember that we gave each word a weighting, which is related to how frequently that word appears after the phrase from our learning text. We need to find a way to choose a word from the list randomly, but still respecting the frequency distribution, or weights.\n\nOne way to think of this is to lay each of the possible choices out on a line, with the space or length of each choice proportional to its weight.\n\nThen we can choose a random point on the line. Whatever word block that random choice lands in, is the word we choose. This way, we are more likely to land on a word with a larger weight, because it takes up more of the line. So we can still choose randomly (i.e. not always return the same word), but still respect the word frequency distribution of natural language.\n\nWe may understand the principle, but how do we do this in code? When we lay out all the words end to end, sized by their weights, we are creating a line with length equal to the sum of all the word weights.", "start_char_idx": 2484, "end_char_idx": 6685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e20735d-9bb9-4f32-a344-b573b12211f4": {"__data__": {"id_": "6e20735d-9bb9-4f32-a344-b573b12211f4", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "8d715231bc49bfaea69af4b71785c513792c7b0713c7f450d47abb3318ef354f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43b9f65c-88ca-40f2-ad82-0be6f7bc61fe", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "f9aad8c73bf7a2fcc12706652fdc4280f7b3e1b6cdb9249377afee71dd78d9eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0067c09-8c22-4dd9-b955-24e752a1ffbc", "node_type": "1", "metadata": {}, "hash": "c8c40894d5a8d58ed2fd3304a9843caea9be5040947197ea9e045a60a4932cb0", "class_name": "RelatedNodeInfo"}}, "hash": "66d45d962b4a0ed01d599135a1902e7275f448cdac702fd52452465b98bb52a5", "text": "We need to find a way to choose a word from the list randomly, but still respecting the frequency distribution, or weights.\n\nOne way to think of this is to lay each of the possible choices out on a line, with the space or length of each choice proportional to its weight.\n\nThen we can choose a random point on the line. Whatever word block that random choice lands in, is the word we choose. This way, we are more likely to land on a word with a larger weight, because it takes up more of the line. So we can still choose randomly (i.e. not always return the same word), but still respect the word frequency distribution of natural language.\n\nWe may understand the principle, but how do we do this in code? When we lay out all the words end to end, sized by their weights, we are creating a line with length equal to the sum of all the word weights. Then, when we choose a random point on the line, it is equivalent to choosing a random number between 0 and the sum of all the weights. To find the word \"under\" the point, we can run through our word list again, and \"add\" each word weight until we match our randomly chosen number. This type of algorithm is known as a weighted random choice algorithm, and there are many ways to implement it.\n\nThis sounds like a job for another function. Let's create a function that takes in a weighted word list, and implements the algorithm above:\n\nfunction\n\nchoose_word_weighted\n\nword_list\n\n// Get an array of all the words in the word list object,\n\n// so we can run through each and get their weights\n\nvar\n\nkeys\n\nObject\n\nkeys\n\nword_list\n\n// Get the sum of all the weights\n\nlet\n\nsum_of_weights\n\nkeys\n\nforEach\n\nkey\n\n=>\n\nsum_of_weights\n\n+=\n\nword_list\n\nkey\n\n// Math.random() returns a number from 0 to 1,\n\n// so we scale it up the sum of the weights\n\nlet\n\nrandom\n\nMath\n\nrandom\n\nsum_of_weights\n\n// Go through the words one by one, and subtract its weight from\n\n// our random number. When we reach 0 or below,\n\n// that is the word we choose\n\nlet\n\ncurr_word\n\n\"\"\n\nkeys\n\nevery\n\nword\n\n=>\n\ncurr_word\n\nword\n\nrandom\n\n=\n\nword_list\n\nword\n\nreturn\n\nrandom\n\nreturn\n\ncurr_word\n\nThe first code line var keys = Object.keys(word_list); uses a built-in JavaScript function from the base Object to get all the keys (the words in our likely list), and return them as an array. This allows us to use this array to iterate over, and query our word list object word by word for each weight.\n\nYou'll notice in the last part of the function, we subtract word weights from our random point. This is equivalent to adding word weights until we reach the random number. It just saves us another variable. You'll also notice that we use keys.every() instead of the more usual keys.forEach. This is because (despite its name), every allows us to break out early from the loop when we find the word that is under our random point, whereas forEach does not allow a break early.\n\nNow we can choose a word with weighted randomness. Let's complete our suggest_word function we started. We just need to call our choose_word_weighted function, so let's update it:\n\nfunction\n\nsuggest_word\n\nstart_phrase\n\nword_map\n\nlet\n\nword_list\n\nword_map\n\nstart_phrase\n\nlet\n\nsuggested_word\n\nchoose_word_weighted\n\nword_list\n\nreturn\n\nsuggested_word\n\nPutting it All Together\u200b\n\nWe've made all the parts. Now let's put it all together and see how it works. We need to:\n\nRead all the books in.\n\nBuild the map.\n\nTest a phrase to complete.\n\nWe also need to set a depth, or the number of words in our initial phrases that we want to predict off of. Aim for two or three; any more than that and the phrases become very unique and we may not have enough data to have seen all of those combinations.", "start_char_idx": 5836, "end_char_idx": 9503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0067c09-8c22-4dd9-b955-24e752a1ffbc": {"__data__": {"id_": "a0067c09-8c22-4dd9-b955-24e752a1ffbc", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "8d715231bc49bfaea69af4b71785c513792c7b0713c7f450d47abb3318ef354f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e20735d-9bb9-4f32-a344-b573b12211f4", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "66d45d962b4a0ed01d599135a1902e7275f448cdac702fd52452465b98bb52a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ebf5a32-7913-4a92-a4a4-bd77224526ac", "node_type": "1", "metadata": {}, "hash": "795f799ce3fc4799771830da722d75ce5265eff7ad8c8a5fceb79d0d0578ff4a", "class_name": "RelatedNodeInfo"}}, "hash": "c8c40894d5a8d58ed2fd3304a9843caea9be5040947197ea9e045a60a4932cb0", "text": "Now we can choose a word with weighted randomness. Let's complete our suggest_word function we started. We just need to call our choose_word_weighted function, so let's update it:\n\nfunction\n\nsuggest_word\n\nstart_phrase\n\nword_map\n\nlet\n\nword_list\n\nword_map\n\nstart_phrase\n\nlet\n\nsuggested_word\n\nchoose_word_weighted\n\nword_list\n\nreturn\n\nsuggested_word\n\nPutting it All Together\u200b\n\nWe've made all the parts. Now let's put it all together and see how it works. We need to:\n\nRead all the books in.\n\nBuild the map.\n\nTest a phrase to complete.\n\nWe also need to set a depth, or the number of words in our initial phrases that we want to predict off of. Aim for two or three; any more than that and the phrases become very unique and we may not have enough data to have seen all of those combinations.\n\nOur completed code, with the above added in, should look like this:\n\nconst\n\nfs\n\nrequire\n\n\"fs\"\n\nconst\n\ndepth\n\nlet\n\nall_words\n\nreadFilesIntoWordArray\n\n\"hgwells.txt\"\n\n\"franklin.txt\"\n\n\"dickens.txt\"\n\nlet\n\nmap\n\nbuildMap\n\nall_words\n\ndepth\n\nlet\n\ninitial_phrase\n\n\"and then I\"\n\nlet\n\noutput\n\nsuggest_word\n\ninitial_phrase\n\nmap\n\nconsole\n\nlog\n\ninitial_phrase\n\n\": \"\n\noutput\n\nfunction\n\nsuggest_word\n\nstart_phrase\n\nword_map\n\nlet\n\nword_list\n\nword_map\n\nstart_phrase\n\nlet\n\nsuggested_word\n\nchoose_word_weighted\n\nword_list\n\nreturn\n\nsuggested_word\n\nfunction\n\nchoose_word\n\nword_list\n\nvar\n\nkeys\n\nObject\n\nkeys\n\nword_list\n\nvar\n\nword\n\nkeys\n\nkeys\n\nlength\n\nMath\n\nrandom\n\n<<\n\nreturn\n\nword\n\nfunction\n\nchoose_word_weighted\n\nword_list\n\n// Get an array of all the words in the word list object,\n\n// so we can run through each and get their weights\n\nvar\n\nkeys\n\nObject\n\nkeys\n\nword_list\n\n// Get the sum of all the weights\n\nlet\n\nsum_of_weights\n\nkeys\n\nforEach\n\nkey\n\n=>\n\nsum_of_weights\n\n+=\n\nword_list\n\nkey\n\n// Math.random() returns a number from 0 to 1,\n\n// so we scale it up the sum of the weights\n\nlet\n\nrandom\n\nMath\n\nrandom\n\nsum_of_weights\n\n// Go through every word, and subtract its weight from\n\n// our random number. When we reach 0 or below,\n\n// that is the word we choose\n\nlet\n\ncurr_word\n\n\"\"\n\nkeys\n\nevery\n\nword\n\n=>\n\ncurr_word\n\nword\n\nrandom\n\n=\n\nword_list\n\nword\n\nreturn\n\nrandom\n\nreturn\n\ncurr_word\n\n/*\n\nRuns through the list, gets the next n-1 words, and maps it to the n+1 word\n\n/\n\nfunction\n\nbuildMap\n\ntokens\n\ndepth\n\nlet\n\nmap\n\n// for each entry in the tokens\n\nfor\n\nlet\n\nindex\n\nindex\n\ntokens\n\nlength\n\ndepth\n\nindex\n\n++\n\n//create a phrase from the current token and the next depth-1 number of tokens\n\nlet\n\nphrase\n\n\"\"\n\nfor\n\nlet\n\ndepthIndex\n\ndepthIndex\n\ndepth\n\ndepthIndex\n\n++\n\nconst\n\ncurr_word\n\ntokens\n\ndepthIndex\n\nindex\n\nphrase\n\nphrase\n\n\" \"\n\ncurr_word\n\n// Get rid of any extra space we added in when constructing the phrase from tokens\n\nphrase\n\nphrase\n\ntrimStart\n\n//if the phrase doesn't already exist in the map\n\n//    add the phrase to the map, and add a blank\n\nif\n\nmap\n\nphrase\n\nmap\n\nphrase\n\n// Gets the next word after the phrase\n\nlet\n\nnext_word\n\ntokens\n\nindex\n\ndepth\n\n// See if the next word exists in the phrase word list\n\n// If it doesn't already exist in the possible next word list, add it in, and set weight to 1\n\n// if it does exist, just increment the weight\n\nlet\n\nnext_word_list\n\nmap\n\nphrase\n\nif\n\nnext_word_list\n\nnext_word\n\nnext_word_list\n\nnext_word\n\nelse\n\nnext_word_list\n\nnext_word\n\n++\n\nreturn\n\nmap\n\nfunction\n\nreadFilesIntoWordArray\n\nfilenames\n\nlet\n\ndata\n\n\"\"\n\nfilenames\n\nforEach\n\nfile\n\n=>\n\ndata\n\ndata\n\n\" \"\n\nfs\n\nreadFileSync\n\nfile\n\n\"utf8\"\n\n// remove newlines\n\ndata\n\ndata\n\nreplace\n\n\\r?\\n|\\r\n\n\" \"\n\n// Put spaces around each special character/punctuation,\n\n// so that when we split on spaces, they come out as their own tokens,\n\n// disconected from surrounding words\n\nconst\n\nreplacements\n\n\",\"\n\n\".\"\n\n\":\"\n\n\"!\"\n\n\"?\"", "start_char_idx": 8717, "end_char_idx": 12385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ebf5a32-7913-4a92-a4a4-bd77224526ac": {"__data__": {"id_": "3ebf5a32-7913-4a92-a4a4-bd77224526ac", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "8d715231bc49bfaea69af4b71785c513792c7b0713c7f450d47abb3318ef354f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0067c09-8c22-4dd9-b955-24e752a1ffbc", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "c8c40894d5a8d58ed2fd3304a9843caea9be5040947197ea9e045a60a4932cb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da88b920-6a3a-46cf-87b1-6bed7803d573", "node_type": "1", "metadata": {}, "hash": "806ba9c011745ddcb2dab99053773db9f9f9bcc5e3ceacaba5ffb9b6bd4a8cd8", "class_name": "RelatedNodeInfo"}}, "hash": "795f799ce3fc4799771830da722d75ce5265eff7ad8c8a5fceb79d0d0578ff4a", "text": "\":\"\n\n\"!\"\n\n\"?\"\n\n'\"'\n\n\"\u201c\"\n\n\"\u201d\"\n\n\";\"\n\n\"(\"\n\n\")\"\n\n\"-\"\n\n\"_\"\n\nreplacements\n\nforEach\n\nvalue\n\n=>\n\ndata\n\ndata\n\nreplace\n\nRegExp\n\n\"\\\\\"\n\nvalue\n\n\"g\"\n\n\" \"\n\nvalue\n\n\" \"\n\n// Split on spaces to get each word by itself, indexed.\n\nvar\n\nword_array\n\ndata\n\nsplit\n\n\" \"\n\n// remove all pure whitespace entries\n\nword_array\n\nword_array\n\nfilter\n\nword\n\n=>\n\nword\n\ntrim\n\nlength\n\n!=\n\nreturn\n\nword_array\n\nRun the project by clicking the big RUN > button at the top centre of the repl, and see what you get back. Here is an example:\n\nCan We Do More?\u200b\n\nThis is pretty good for an engine, which we could integrate into a text/chat app, word processor, or another project. But can we do something else right now just for fun?\n\nWhat if we keep feeding the last depth number of words in the phrase back into the autocomplete, to see if it can come up with a complete sentence? You might have been this game on social media where you are asked to start a comment with \"I want\" and then keep selecting autocomplete words to come up with a nonsense or comical sentence.\n\nLet's create a new function, create_sentence that does this:\n\n// Creates a new sequence of words, of max length, given a starting phrase\n\nfunction\n\ncreate_sentence\n\nstart_phrase\n\nword_map\n\nsentence_length\n\ndepth\n\nlet\n\nsentence\n\n\"\"\n\nfor\n\nlet\n\nword_count\n\nword_count\n\nsentence_length\n\nword_count\n\n++\n\nlet\n\nnext_word\n\nchoose_word_weighted\n\nword_map\n\nstart_phrase\n\nsentence\n\nsentence\n\nnext_word\n\n\" \"\n\ntokenized_phrase\n\nstart_phrase\n\nsplit\n\n\" \"\n\nstart_phrase\n\n\"\"\n\nfor\n\nlet\n\ndepth\n\n++\n\nstart_phrase\n\nstart_phrase\n\ntokenized_phrase\n\n\" \"\n\nstart_phrase\n\nstart_phrase\n\nnext_word\n\nreturn\n\nsentence\n\nAs for the suggest_word function, we have parameters for the start_phrase and for the populated word_map. Then there is also an input for sentence_length, which is basically how many rounds to run the autocomplete. We also pass in depth, so that this function knows how many words it must use as an initial phrase for each autocomplete round.\n\nThe function then sets up a loop to run the autocomplete for sentence_length times. It starts off the same as the suggest_word function by calling choose_weighted_word to get the next word for the given phrase. Then it concatenates that word to a sentence string.\n\nThe next few lines then split up the initial phrase into individual tokens, takes the last depth-1 words/tokens, and appends the newly chosen word to the end to make a new initial phrase of depth length. Then the cycle starts again, until we have generated a bit of text which is sentence_length long.\n\nThis is going to give some interesting results! Add the function above to your code, and then modify the initial code to call it:\n\nlet\n\ninitial_phrase\n\n\"and then I\"\n\nlet\n\nsentence\n\ncreate_sentence\n\ninitial_phrase\n\nmap\n\n50\n\ndepth\n\nconsole\n\nlog\n\ninitial_phrase\n\n\": \"\n\nsentence\n\nThis is an example output.\n\nIt seems like real language, but it's still completely nonsensical and a fun way to generate random stories. Try with varying parameters \u2013 initial phrases, sentence length and parameters.\n\nOther Engine Applications\u200b\n\nWe can use our engine for other projects. The engine or model we created is known as a type of Markov Chain. A Markov chain is used as a model when we have an environmental 'state', which can transition to other states through a variety of actions. We call something 'Markovian' when the probability of each action, or event, can be sufficiently modelled by only knowing the current state, and not taking into account previous states, or history.\n\nIn our case, a state is a phrase of a certain length, and the action is the likely word to pick, leading to a new 'state' or phrase.\n\nOther things that can be modelled quite well with Markov chains include games like Tic-Tac-Toe, or Chess, where the current state is easy to define, and there is a finite list of possible actions for each state (although in Chess, this can get rather large).\n\nThings to Try Next\u200b\n\nThere are some ways to improve this engine:\n\nIf it hasn't seen a particular initial phrase, the code will crash. It would be good to add a check to see if the phrase doesn't exist. It could then return an error code or an empty suggestion, rather than crashing.\n\nIt could be cool to make the engine interactive.", "start_char_idx": 12372, "end_char_idx": 16594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da88b920-6a3a-46cf-87b1-6bed7803d573": {"__data__": {"id_": "da88b920-6a3a-46cf-87b1-6bed7803d573", "embedding": null, "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68", "node_type": "4", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "8d715231bc49bfaea69af4b71785c513792c7b0713c7f450d47abb3318ef354f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ebf5a32-7913-4a92-a4a4-bd77224526ac", "node_type": "1", "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}, "hash": "795f799ce3fc4799771830da722d75ce5265eff7ad8c8a5fceb79d0d0578ff4a", "class_name": "RelatedNodeInfo"}}, "hash": "806ba9c011745ddcb2dab99053773db9f9f9bcc5e3ceacaba5ffb9b6bd4a8cd8", "text": "In our case, a state is a phrase of a certain length, and the action is the likely word to pick, leading to a new 'state' or phrase.\n\nOther things that can be modelled quite well with Markov chains include games like Tic-Tac-Toe, or Chess, where the current state is easy to define, and there is a finite list of possible actions for each state (although in Chess, this can get rather large).\n\nThings to Try Next\u200b\n\nThere are some ways to improve this engine:\n\nIf it hasn't seen a particular initial phrase, the code will crash. It would be good to add a check to see if the phrase doesn't exist. It could then return an error code or an empty suggestion, rather than crashing.\n\nIt could be cool to make the engine interactive. Try adding a console interface like readline module to prompt for a phrase/input and show the output, allowing you to try multiple phrases in one session.\n\nTry save the populated map to a data store, so it doesn't have to be re-trained every time you run the program. This would allow you to continually add new books and language examples, making the engine even better. There is a Replit database you can use for this.\n\n< Previous: Online store checkout with Stripe\n\nNext: Chat app using Node.js >\n\nWas this helpful?\n\nLast updated on Aug 31, 2023\n\nOverview and Requirements\n\nCreating a New Project\n\nFinding Training Text\n\nReading in the Books\n\nCreating the Data Structure\n\nCompleting a Phrase\n\nPutting it All Together\n\nCan We Do More?\n\nOther Engine Applications\n\nThings to Try Next", "start_char_idx": 15868, "end_char_idx": 17378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"8147d53b-0c7f-4ba8-b80e-4d15d9233c68": {"node_ids": ["f3052cdf-9dd3-46c6-bd7f-cf497590c62a", "43b9f65c-88ca-40f2-ad82-0be6f7bc61fe", "6e20735d-9bb9-4f32-a344-b573b12211f4", "a0067c09-8c22-4dd9-b955-24e752a1ffbc", "3ebf5a32-7913-4a92-a4a4-bd77224526ac", "da88b920-6a3a-46cf-87b1-6bed7803d573"], "metadata": {"path": "/home/runner/replit-docs-qa-with-llamaindex/docs.replit.com/tutorials/nodejs/predictive-text-engine.html"}}}, "docstore/metadata": {"f3052cdf-9dd3-46c6-bd7f-cf497590c62a": {"doc_hash": "6b1498aff70c18e85d5c75fa41b3c99f46ff183e375ac52aa1e253ce61ca89f9", "ref_doc_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68"}, "43b9f65c-88ca-40f2-ad82-0be6f7bc61fe": {"doc_hash": "f9aad8c73bf7a2fcc12706652fdc4280f7b3e1b6cdb9249377afee71dd78d9eb", "ref_doc_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68"}, "6e20735d-9bb9-4f32-a344-b573b12211f4": {"doc_hash": "66d45d962b4a0ed01d599135a1902e7275f448cdac702fd52452465b98bb52a5", "ref_doc_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68"}, "a0067c09-8c22-4dd9-b955-24e752a1ffbc": {"doc_hash": "c8c40894d5a8d58ed2fd3304a9843caea9be5040947197ea9e045a60a4932cb0", "ref_doc_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68"}, "3ebf5a32-7913-4a92-a4a4-bd77224526ac": {"doc_hash": "795f799ce3fc4799771830da722d75ce5265eff7ad8c8a5fceb79d0d0578ff4a", "ref_doc_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68"}, "da88b920-6a3a-46cf-87b1-6bed7803d573": {"doc_hash": "806ba9c011745ddcb2dab99053773db9f9f9bcc5e3ceacaba5ffb9b6bd4a8cd8", "ref_doc_id": "8147d53b-0c7f-4ba8-b80e-4d15d9233c68"}}}